#!/bin/bash

# install a datalad data repository specific to one subject
# run neuroimaging analysis pipeline (fmriprep/mindboggle/mriqc)

# push results to S3 bucket

DATASET=$2

# for use with c4/5 2XL instance type
AWSBATCH_CPUS=${AWSBATCH_CPUS:-8}
AWSBATCH_MEM_MB=${AWSBATCH_MEM_MB:-14000}
S3_FS_DIR=${S3_FS_DIR:-""}
PUSH_FS_RESULTS=0

S3_SYNC_ARGS="--acl public-read"

if [[ -n ${AWS_BATCH_JOB_ARRAY_INDEX} ]]; then
  # job array
  subjidx=$(expr ${AWS_BATCH_JOB_ARRAY_INDEX} + 3)
  SUBJECT=${!subjidx}
else
  # single job
  SUBJECT=$3
fi

if [[ -z ${DATASET} ]] || [[ -z ${SUBJECT} ]]; then
  echo "Dataset or subject not properly specified"
  exit 1
fi

if [[ -z ${S3_BUCKET} ]]; then
  echo "No S3 Bucket provided"
  exit 1
fi

if [[ -z ${JOBTYPE} ]]; then
  echo "JOBTYPE not provided - supported args (fmriprep, mindboggle, mriqc)"
fi

# account for openneuro datasets
SITE=$(basename ${DATASET})
if [[ ${SITE} = ds* ]]; then
  S3_OUTDIR="${S3_BUCKET}/${SITE}/derivatives"
else
  S3_OUTDIR="${S3_BUCKET}/derivatives"
fi

BIDSDIR="/working/data"
OUTDIR="/working/derivatives"
WORKDIR="/working/scratch"

# make data directory
if [ ! -d ${BIDSDIR} ]; then
  mkdir -p ${BIDSDIR}
fi

# ensure derivatives exists
if [ ! -d ${OUTDIR} ]; then
    mkdir -p ${OUTDIR}
fi

# make working directory
if [ ! -d ${WORKDIR} ]; then
    mkdir -p ${WORKDIR}
fi

## BEGIN DATA FETCHING
echo "Fetching data with datalad"

# first install the remote repository
datalad install -r -s ${DATASET} ${BIDSDIR}
# now fetch only files necessary for single subject fmriprep
# do not need func if running mindboggle only...
case "${JOBTYPE}" in
  mindboggle)
    datalad get -r -J ${AWSBATCH_CPUS} ${BIDSDIR}/${SUBJECT}/anat/* ${BIDSDIR}/${SUBJECT}/*/anat/* data/*.json || true
    ;;
  *)
    datalad get -r -J ${AWSBATCH_CPUS} ${BIDSDIR}/${SUBJECT}/{func,anat}/* ${BIDSDIR}/${SUBJECT}/*/{func,anat}/* data/*.json || true
    ;;
esac

exists_s3 ()
{
  aws s3 ls ${1} > /dev/null
  return $?
}

if [ "${JOBTYPE}" = "mriqc" ]; then
  AWSBATCH_MEM_GB=$(expr ${AWSBATCH_MEM_MB} / 1000)
  CMD="mriqc ${BIDSDIR} ${OUTDIR}/mriqc participant \
      --participant_label ${SUBJECT:4} --n_procs ${AWSBATCH_CPUS} \
      --mem_gb ${AWSBATCH_MEM_GB} -w ${WORKDIR} --verbose-reports --ica"

  # rerun with ica
  # if exists_s3 ${S3_OUTDIR}/mriqc/${SUBJECT}; then
  #   CMD=":"
  # fi

elif [ "${JOBTYPE}" = "fmriprep" ]; then

  CMD="fmriprep ${BIDSDIR} ${OUTDIR} participant --cifti-output \
      --participant_label ${SUBJECT:4} --nthreads ${AWSBATCH_CPUS} \
      --mem_mb ${AWSBATCH_MEM_MB} --output-space template fsaverage5 \
      --template-resampling-grid 2mm --ignore slicetiming -w ${WORKDIR} \
      --use-aroma --omp-nthreads 1 --ignore-aroma-denoising-errors \
      --skip_bids_validation --fs-license-file /tmp/.fs_license.txt"

  # check if already run
  if exists_s3 ${S3_OUTDIR}/fmriprep/${SUBJECT}; then
    CMD=":"
  elif [[ -n ${S3_FS_DIR} ]]; then
    aws s3 cp ${S3_FS_DIR}/${SUBJECT}/ ${OUTDIR}/freesurfer/${SUBJECT} --recursive
    PUSH_FS_RESULTS=1
  fi

elif [ "${JOBTYPE}" = "mindboggle" ]; then

  T1PATH=($(find ${BIDSDIR} -name "${SUBJECT}*_T1w.n*"))
  if [ ${#T1PATH} -gt 1 ]; then
    echo "Multiple T1w images found for ${SUBJECT}"
    T1PATH=${T1PATH[0]}
    echo "Using T1w image: ${T1PATH}"
  fi

  [ -f "${T1PATH}" ] || { echo "T1 not found for ${SUBJECT}"; exit 1; }

  CMD="mindboggle123 ${T1PATH} --id ${SUBJECT} --out ${OUTDIR}/mindboggle \
      --working ${WORKDIR} --plugin MultiProc --prov --ants_segN 20"

  if [[ -n ${ANTS_SEG_FUSION} ]]; then
    CMD="${CMD} --ants_seg fusion"
  else
    CMD="${CMD} --ants_seg quick"  # default - be explicit
  fi

  # Use T2 if possible
  T2PATH=${T1PATH/_T1w./_T2w.}
  [ ! -f ${T2PATH} ] || CMD="${CMD} --fs_T2image ${T2PATH}"


  # check if subject ID is found in the S3 output
  # possible return codes:
  # 100 - mindboggle completed, skip entirely
  # 50 - freesurfer completed, skip recon-all
  # 0 - no existing output found
  previous_mb_run()
  {
    exists_s3 ${S3_OUTDIR}/mindboggle/mindboggled/${1}
    if [ $? -eq 0 ]; then
      return 100
    fi
    exists_s3 ${S3_OUTDIR}/mindboggle/freesurfer_subjects/${1}
    if [ $? -eq 0 ]; then
      return 50
    fi
    return 0
  }

  # dedicate 1 thread for FreeSurfer recon-all
  ANTS_THREADS=$(expr ${AWSBATCH_CPUS} - 1)

  # Avoid redundant running
  previous_mb_run ${SUBJECT}
  case $? in
    100)
      echo "Existing mindboggle run found, skipping"
      CMD=":"
      ;;
    50)
      echo "Existing FreeSurfer recon found, pulling..."
      aws s3 cp ${S3_OUTDIR}/mindboggle/freesurfer_subjects/${SUBJECT} \
                ${OUTDIR}/mindboggle/freesurfer_subjects/${SUBJECT} --recursive
      CMD="${CMD} --skip_freesurfer --ants_num_threads ${AWSBATCH_CPUS} \
          --mb_num_threads ${AWSBATCH_CPUS}"
      ;;
    0)
      CMD="${CMD} --fs_openmp 1 --ants_num_threads ${ANTS_THREADS} \
          --mb_num_threads ${ANTS_THREADS}"
      ;;
  esac

  # dedicate 1 thread for FreeSurfer recon-all
  ANTS_THREADS=$(expr ${AWSBATCH_CPUS} - 1)

  # Avoid redundant running
  previous_mb_run ${SUBJECT}
  case $? in
    100)
      echo "Existing mindboggle run found, skipping"
      CMD=":"
      ;;
    50)
      echo "Existing FreeSurfer recon found, pulling..."
      aws s3 cp ${S3_OUTDIR}/mindboggle/freesurfer_subjects/${SUBJECT} \
                ${OUTDIR}/mindboggle/freesurfer_subjects/${SUBJECT} --recursive
      CMD="${CMD} --skip_freesurfer --ants_num_threads ${AWSBATCH_CPUS} \
          --mb_num_threads ${AWSBATCH_CPUS}"
      ;;
    0)
      CMD="${CMD} --fs_openmp 1 --ants_num_threads ${ANTS_THREADS} \
          --mb_num_threads ${ANTS_THREADS}"
      ;;
  esac

  # run mindboggle
  printf "Command:\n${CMD}\n\n"
  eval ${CMD}

  # push to S3 to allow clearing up space
  aws s3 sync ${OUTDIR}/mindboggle ${S3_OUTDIR}/mindboggle ${S3_SYNC_ARGS} --exclude "fsaverage"
  rm -rf ${OUTDIR}/mindboggle

  # check if previous simple_workflow run exists
  exists_s3 ${S3_OUTDIR}/simple_workflow/${SUBJECT}
  if [ $? -eq 0 ]; then
    echo "Exisiting simple workflow output found"
    CMD=":"
  else
    # and queue up simple workflow as well
    CMD="/working/simple_workflow.py -i ${T1PATH} -s ${SUBJECT} \
        -o ${OUTDIR}/simple_workflow -w ${WORKDIR}"
  fi

else  # other job types
  echo "${JOBTYPE} is not currently supported"
  exit 1
fi

printf "Command:\n${CMD}\n\n"
eval ${CMD}

# only push FS if not found previously
if [ ${PUSH_FS_RESULTS} -eq 0 ]; then
  aws s3 sync ${OUTDIR}/ ${S3_OUTDIR}/ ${S3_SYNC_ARGS}
else
  aws s3 sync ${OUTDIR}/fmriprep/ ${S3_OUTDIR}/fmriprep/ ${S3_SYNC_ARGS}
fi

# save any mriqc outputs
aws s3 sync ${OUTDIR}/mriqc/ ${S3_OUTDIR}/mriqc/ ${S3_SYNC_ARGS}

# or simple workflow outputs
aws s3 sync ${OUTDIR}/simple_workflow ${S3_OUTDIR}/simple_workflow ${S3_SYNC_ARGS}

# save any provenance files
aws s3 cp ${WORKDIR}/ ${S3_OUTDIR}/provenance/${SUBJECT}/ --recursive --exclude "*" --include "workflow_provenance*" ${S3_SYNC_ARGS}

# and any crashfiles along the way
aws s3 cp ${WORKDIR}/ ${S3_BUCKET}/crashes/${SUBJECT}/ --recursive --exclude "*" --include "crash*" ${S3_SYNC_ARGS}
exit 0
